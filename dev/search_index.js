var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"CurrentModule = XGBoost","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#Data-Input","page":"API","title":"Data Input","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DMatrix\nload(::Type{DMatrix}, ::AbstractString)\nsave(::DMatrix, ::AbstractString)\nsetfeaturenames!\ngetfeaturenames\nsetinfo!\nsetinfos!\nsetlabel!\ngetinfo\nslice\nnrows\nncols\nsize(::DMatrix)\ngetlabel\ngetweights\nsetfeatureinfo!\ngetfeatureinfo\nsetproxy!\nDataIterator\nfromiterator","category":"page"},{"location":"api/#XGBoost.DMatrix","page":"API","title":"XGBoost.DMatrix","text":"DMatrix <: AbstractMatrix{Union{Missing,Float32}}\n\nData structure for storing data which can be understood by an xgboost Booster. These can store both features and targets.  Values of the DMatrix can be accessed as with any other AbstractMatrix, however doing so causes additional allocations.  Performant indexing and matrix operation code should not use DMatrix directly.\n\nAside from a primary array, the DMatrix object can have various \"info\" fields associated with it. Training target variables are stored as a special info field with the name label, see setinfo! and setinfos!.  These can be retrieved with getinfo and getlabel.\n\nNote that the xgboost library internally uses Float32 to represent all data, so input data is automatically copied unless provided in this format.  Unfortunately because of the different representations used by C and Julia, any non Transpose matrix will also be copied.\n\nOn missing Values\n\nXgboost supports training on missing data.  Such data is simply omitted from tree splits.  Because the DMatrix is internally a Float32 matrix, libxgboost uses a settable default value to represent missing values, see the missing_value keyword argument below (default NaN32).  This value is used only on matrix construction.  This will cause input matrix elements to ultimately be converted to missing.  The most obvious consequence of this is that NaN32 values will automatically be converted to missing with default arguments.  The provided constructors ensure that missing values will be preserved.\n\nTL;DR: DMatrix supports missing and NaN's will be converted to missing.\n\nConstructors\n\nDMatrix(X::AbstractMatrix; kw...)\nDMatrix(X::AbstractMatrix, y::AbstractVector; kw...)\nDMatrix((X, y); kw...)\nDMatrix(tbl; kw...)\nDMatrix(tbl, y; kw...)\nDMatrix(tbl, yname::Symbol; kw...)\n\nArguments\n\nX: A matrix that is the primary data wrapped by the DMatrix.  Elements can be missing.   Matrices with Float32 eleemnts do not need to be copied.\ny: Data to assign to the label info field.  This is the target variable used in training.   Can also be set with the label keyword.\ntbl: The input matrix in tabular form.  tbl must satisfy the Tables.jl interface.   If data is passed in tabular form feature names will be set automatically but can   be overriden with the keyword argument.\nyname: If passed a tabular argument tbl, yname is the name of the column which holds the   label data.  It will automatically be omitted from the features.\n\nKeyword Arguments\n\nmissing_value: The Float32 value of elements of input data to be interpreted as missing,   defaults to NaN32.\nlabel: Training target data, this is the same as the y argument above, i.e.   DMatrix(X,y) and DMatrix(X, label=y) are equivalent.\nweight: An AbstractVector of weights for each data point.  This array must have lenght   equal to the number of rows of the main data matrix.\nbase_margin: Sets the global bias for a boosted model trained on this dataset, see   https://xgboost.readthedocs.io/en/stable/prediction.html#base-margin\n\nExamples\n\n(X, y) = (randn(10,3), randn(10))\n\n# the following are all equivalent\nDMatrix(X, y)\nDMatrix((X, y))\nDMatrix(X, label=y)\n\nDMatrix(X, y, feature_names=[\"a\", \"b\", \"c\"])  # explicitly set feature names\n\ndf = DataFrame(A=randn(10), B=randn(10))\nDMatrix(df)  # has feature names [\"A\", \"B\"] but no label\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.load-Tuple{Type{DMatrix}, AbstractString}","page":"API","title":"XGBoost.load","text":"load(DMatrix, fname; silent=true, kw...)\n\nLoad a DMatrix from file with name fname.  The matrix must have been serialized with a call to save(::DMatrix, fname).  If silent the xgboost library will print logs to stdout. Additional keyword arguments are passed to the DMatrix on construction.\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.save-Tuple{DMatrix, AbstractString}","page":"API","title":"XGBoost.save","text":"save(dm::DMatrix, fname; silent=true)\n\nSave the DMatrix to file fname in an opaque (xgboost-specific) serialization format. Will print logs to stdout unless silent.\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.setfeaturenames!","page":"API","title":"XGBoost.setfeaturenames!","text":"setfeaturenames!(dm::DMatrix, names)\n\nSets the names of the features in dm.  This can be used by Booster for reporting. names must be a rank-1 array of strings with length equal to the number of features. Note that this will be set automatically by DMatrix constructors from table objects.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getfeaturenames","page":"API","title":"XGBoost.getfeaturenames","text":"getfeaturenames(dm::DMatrix)\n\nGet the names of features in dm.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setinfo!","page":"API","title":"XGBoost.setinfo!","text":"setinfo!(dm::DMatrix, name, info)\n\nSet DMatrix ancillary info, for example :label or :weight.  name can be a string or a Symbol.  See DMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setinfos!","page":"API","title":"XGBoost.setinfos!","text":"setinfos!(dm::DMatrix; kw...)\n\nMake arbitrarily many calls to setinfo! via keyword arguments.  This function is called by all DMatrix constructors, i.e. DMatrix(X; kw...) is equivalent to setinfos!(DMatrix(X); kw...).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setlabel!","page":"API","title":"XGBoost.setlabel!","text":"setlabel!(dm::DMatrix, y)\n\nSet the label data of dm to y.  Equivalent to setinfo!(dm, \"label\", y).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getinfo","page":"API","title":"XGBoost.getinfo","text":"getinfo(dm::DMatrix, T, name)\n\nGet DMatrix info with name name.  Users must specify the underlying data type due to limitations of the xgboost library.  One must have T<:AbstractFloat to get floating point data (e.g. label, weight), or T<:Integer to get integer data.  The output will be converted to Vector{T} in all cases. name can be either a string or Symbol.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.slice","page":"API","title":"XGBoost.slice","text":"slice(dm::DMatrix, idx; kw...)\n\nCreate a new DMatrix out of the subset of rows of dm given by indices idx. For performance reasons it is recommended to take slices before converting to DMatrix. Additional keyword arguments are passed to the newly constructed slice.\n\nThis can also be called via Base.getindex, for example, the following are equivalent\n\nslice(dm, 1:4)\ndm[1:4, :]  # second argument *must* be `:` as column slices are not supported.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.nrows","page":"API","title":"XGBoost.nrows","text":"nrows(dm::DMatrix)\n\nReturns the number of rows of the DMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.ncols","page":"API","title":"XGBoost.ncols","text":"ncols(dm::DMatrix)\n\nReturns the number of columns of the DMatrix.  Note that this will only count columns of the main data (the X argument to the constructor).  The value returned is independent of the presence of labels. In particular size(X,2) == ncols(DMatrix(X)).\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.size-Tuple{DMatrix}","page":"API","title":"Base.size","text":"size(dm::DMatrix, [dim])\n\nReturns the size of the primary data of the DMatrix.  Note that this only accounts for the primary data and is independent of whether labels or any other ancillary data are present.  In particular size(X) == size(DMatrix(X)).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.getlabel","page":"API","title":"XGBoost.getlabel","text":"getlabel(dm::DMatrix)\n\nRetrieve the label (training target) data from the DMatrix.  Returns Float32[] if not set.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getweights","page":"API","title":"XGBoost.getweights","text":"getweights(dm::DMatrix)\n\nGet data training weights.  Returns Float32[] if not set.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setfeatureinfo!","page":"API","title":"XGBoost.setfeatureinfo!","text":"setfeatureinfo!(dm::DMatrix, info_name, strs)\n\nSets feature metadata in dm.  Valid options for info_name are \"feature_name\" and \"feature_type\". strs must be a rank-1 array of strings.  See setfeaturenames!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getfeatureinfo","page":"API","title":"XGBoost.getfeatureinfo","text":"getfeatureinfo(dm::DMatrix, info_name)\n\nGet feature info that was set via setfeatureinfo!.  Valid options for info_name are \"feature_name\" and \"feature_type\".  See getfeaturenames.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setproxy!","page":"API","title":"XGBoost.setproxy!","text":"setproxy!(dm::DMatrix, X::AbstractMatrix; kw...)\n\nSet data in a \"proxy\" DMatrix like one created with proxy(DMatrix).  Keyword arguments are set to the passed matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.DataIterator","page":"API","title":"XGBoost.DataIterator","text":"DataIterator\n\nA data structure which wraps an iterator which iteratively provides data for a DMatrix.  This can be used e.g. to aid with loading data into external memory into a DMatrix object that can be used by Booster.\n\nUsers should not typically have to deal with DataIterator directly as it is essentially a wrapper around a normal Julia iterator for the purpose of achieving compatiblity with the underlying xgboost library calls.  See fromiterator for how to construct a DMatrix from an iterator.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.fromiterator","page":"API","title":"XGBoost.fromiterator","text":"fromiterator(DMatrix, itr; cache_prefix=joinpath(tempdir(),\"xgb-cache\"), nthreads=nothing, kw...)\n\nCreate a DMatrix from an iterable object.  itr can be any object that implements Julia's Base iteration protocol.  Objects returned by the iterator must be key-value collections with Symbol keys with X as the main matrix and y as labels.  For example\n\n(X=randn(10,2), y=randn(10))\n\nOther keys will be interpreted as keyword arguments to DMatrix.\n\nWhen this is called XGBoost will start caching data provided by the iterator on disk in a format that it likes.  All cache files generated this way will have a the prefix cache_prefix which is in /tmp by default.\n\nWhat exactly xgboost does with nthreads is a bit mysterious, nothing gives the library's default.\n\nAdditional keyword arguments are passed to a DMatrix constructor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training-and-Prediction","page":"API","title":"Training and Prediction","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"xgboost\nBooster\nupdateone!\nupdate!\npredict\nsetparam!\nsetparams!\ngetnrounds\nload!\nload(::Type{Booster}, ::AbstractString)\nsave(::Booster, ::AbstractString)\nserialize\nnfeatures\ndeserialize!\ndeserialize","category":"page"},{"location":"api/#XGBoost.xgboost","page":"API","title":"XGBoost.xgboost","text":"xgboost(data; num_round=10, watchlist=Dict(), kw...)\nxgboost(data, ‚Ñì‚Ä≤, ‚Ñì‚Ä≥; kw...)\n\nCreates an xgboost gradient booster object on training data data and runs nrounds of training. This is essentially an alias for constructing a Booster with data and keyword arguments followed by update! for nrounds.\n\nwatchlist is a dict the keys of which are strings giving the name of the data to watch and the values of which are DMatrix objects containing the data.\n\nAll other keyword arguments are passed to Booster.  With few exceptions these are model training hyper-parameters, see here for a comprehensive list.\n\nA custom loss function can be privded via its first and second derivatives (‚Ñì‚Ä≤ and ‚Ñì‚Ä≥ respectively). See updateone! for more details.\n\nExamples\n\n(X, y) = (randn(100,3), randn(100))\n\nb = xgboost((X, y), 10, max_depth=10, Œ∑=0.1)\n\nyÃÇ = predict(b, X)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.Booster","page":"API","title":"XGBoost.Booster","text":"Booster\n\nData structure containing xgboost decision trees or other model objects.  Booster is used in all methods for training and predition.\n\nBooster can only consume data from DMatrix objects but most methods can convert provided data implicitly.  Note that Booster does not store any of its input or output data.\n\nSee xgboost which is shorthand for a Booster constructor followed by training.\n\nThe Booster object records all non-default model hyper-parameters set either at construction or with setparam!.  The xgboost library does not support retrieval of such parameters so these should be considered for UI purposes only, they are reported in the deafult show methods of the Booster.\n\nConstructors\n\nBooster(train; kw...)\nBooster(trains::AbstractVector; kw...)\n\nArguments\n\ntrain: Training data.  If not a DMatrix this will be passed to the DMatrix constructor.   For example it can be a training matrix or a training matrix, target pair.\ntrains: An array of objects used as training data, each of which will be passed to a DMatrix   constructor.\n\nKeyword Arguments\n\nAll keyword arguments excepting only those listed below will be interpreted as model parameters, see here for a comprehensive list. Both parameter names and their values must be provided exactly as they appear in the linked documentation.  Model parameters can also be set after construction, see setparam! and setparams!.\n\nfeature_names: Sets the feature names of training data.  This will use the feature names set in the   input data if available (e.g. if tabular data was passed this will use column names).\nmodel_buffer: A buffer (AbstractVector{UInt8} or IO) from which to load an existing booster   model object.\nmodel_file: Name of a file from which to load an existing booster model object, see save.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.updateone!","page":"API","title":"XGBoost.updateone!","text":"updateone!(b::Booster, data; round_number=getnrounds(b)+1,\n           watchlist=Dict(\"train\"=>data), update_feature_names=false\n          )\n\nRun one round of gradient boosting with booster b on data data.  data can be any object that is accepted by a DMatrix constructor.  round_number is the number of the current round and is used for logs only.  Info logs will be printed for training sets in watchlist; keys give the name of that dataset for logging purposes only.\n\n\n\n\n\nupdateone!(b::Booster, data, ‚Ñì‚Ä≤, ‚Ñì‚Ä≥; kw...)\n\nRun one of gradient boosting with a loss function ‚Ñì.  ‚Ñì‚Ä≤ and ‚Ñì‚Ä≥ are the first and second scalar derivatives of the loss function.  For example\n\n‚Ñì(yÃÇ, y) = (yÃÇ - y)^2\n‚Ñì‚Ä≤(yÃÇ, y) = 2(yÃÇ - y)\n‚Ñì‚Ä≥(yÃÇ, y) = 2\n\nwhere the derivatives are with respect to the first argument (the prediction).\n\nOther arguments are the same as they would be provided to other methods of updateone!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.update!","page":"API","title":"XGBoost.update!","text":"update!(b::Booster, data; num_round=1, kw...)\nupdate!(b::Booster, data, ‚Ñì‚Ä≤, ‚Ñì‚Ä≥; kw...)\n\nRun num_round rounds of gradient boosting on Booster b.\n\nThe first and second derivatives of the loss function (‚Ñì‚Ä≤ and ‚Ñì‚Ä≥ respectively) can be provided for custom loss.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.predict","page":"API","title":"XGBoost.predict","text":"predict(b::Booster, data; margin=false, training=false, ntree_limit=0)\n\nUse the model b to run predictions on data.  This will return a Vector{Float32} which can be compared to training or test target data.\n\nIf ntree_limit > 0 only the first ntree_limit trees will be used in prediction.\n\nExamples\n\n(X, y) = (randn(100,3), randn(100))\nb = xgboost((X, y), 10)\n\nyÃÇ = predict(b, X)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setparam!","page":"API","title":"XGBoost.setparam!","text":"setparam!(b::Booster, name, val)\n\nSet a model parameter in the booster.  The complete list of model parameters can be found here.  Any non-default parameters set via this method will be stored so they can be seen in REPL text output, however the xgboost library does not support parameter retrieval. name can be either a string or a Symbol.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setparams!","page":"API","title":"XGBoost.setparams!","text":"setparams!(b::Booster; kw...)\n\nSet arbitrarily many model parameters via keyword arguments, see setparam!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getnrounds","page":"API","title":"XGBoost.getnrounds","text":"getnrounds(b::Booster)\n\nGet the number of rounds run by the Booster object.  Normally this will correspond to the total number of trees stored in the Booster.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.load!","page":"API","title":"XGBoost.load!","text":"load!(b::Booster, file_or_buffer)\n\nLoad a serialized Booster object from a file or buffer into an existing model object. file_or_buffer can be a string giving the name of the file to load from, a AbstractVector{UInt8} buffer, or an IO.\n\nThis should load models stored via save (not serialize which may give incompatible buffers).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.load-Tuple{Type{Booster}, AbstractString}","page":"API","title":"XGBoost.load","text":"load(Booster, file_or_buffer)\n\nLoad a saved Booster model object from a file or buffer. file_or_buffer can be a string giving the name of the file to load from, an AbstractVector{UInt8} buffer or an IO.\n\nThis should load models stored via save (not serialize which may give incompatible buffers).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.save-Tuple{Booster, AbstractString}","page":"API","title":"XGBoost.save","text":"save(b::Booster, fname)\nsave(b::Booster, Vector{UInt8}; format=\"json\")\nsave(b::Booster, io::IO; format=\"json\")\n\nSave the Booster object.  This saves to formats which are intended to be stored on disk but the formats used are a lot zanier than those used by deserialize. A model saved with this function can be retrieved with load or load!.\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.serialize","page":"API","title":"XGBoost.serialize","text":"serialize(b::Booster)\n\nSerialize the model b into an opaque binary format.  Returns a Vector{UInt8}. The output of this function can be loaded with deserialize.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.nfeatures","page":"API","title":"XGBoost.nfeatures","text":"nfeatures(b::Booster)\n\nGet the number of features on which b is being trained.  Note that this can return nothing if the Booster object is uninitialized (was created with no data arguments).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.deserialize!","page":"API","title":"XGBoost.deserialize!","text":"deserialize!(b::Booster, buf)\n\nDeserialize a buffer created with serialize to the provided Booster object.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.deserialize","page":"API","title":"XGBoost.deserialize","text":"deserialize(Booster, buf, data=[]; kw...)\n\nDeserialize the data in buffer buf to a new Booster object.  The data in buf should have been created with serialize.  data and keyword arguments are sent to a Booster constructor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Introspection","page":"API","title":"Introspection","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"trees\nimportancetable\nimportance\nimportancereport\nNode\ndump\ndumpraw","category":"page"},{"location":"api/#XGBoost.trees","page":"API","title":"XGBoost.trees","text":"trees(b::Booster; with_stats=true)\n\nReturn all trees of the model of the Booster b as Node objects.  The output of this function is a Vector of Nodes each representing the root of a separate tree.\n\nIf with_stats the output Node objects will contain the computed statistics gain and cover.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importancetable","page":"API","title":"XGBoost.importancetable","text":"importancetable(b::Booster)\n\nReturn a Table.jl compatible table (named tuple of Vectors) giving a summary of all available feature importance statistics for b.  This table is mainly intended for display purposes, see importance for a more direct way of retrieving importance statistics. See importancereport for a convenient display of this table.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importance","page":"API","title":"XGBoost.importance","text":"importance(b::Booster, type=\"gain\")\n\nCompute feature importance metric from a trained Booster. Valid options for type are\n\n\"gain\"\n\"weight\"\n\"cover\"\n\"total_gain\"\n\"total_cover\"\n\nThe output is an OrderedDict with keys corresponding to feature names and values corresponding to importances.  The importances are always returned as Vectors, typically with length 1 but possibly longer in multi-class cases.  If feature names were not set the keys of the output dict will be integers giving the feature column number.  The output will be sorted with the highest importance feature listed first and the lowest importance feature listed last.\n\nSee importancetable for a way to generate a tabular summary of all available feature importances and importancereport for a convenient text display of it.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importancereport","page":"API","title":"XGBoost.importancereport","text":"importancereport(b::Booster)\n\nShow a convenient text display of the table output by importancetable.  This is intended entirely for display purposes, see importance for how to retrieve feature importance statistics directly.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.Node","page":"API","title":"XGBoost.Node","text":"Node\n\nA data structure representing a node of an XGBoost tree. These are constructed from the dicts returned by dump.\n\nNodes satisfy the AbstractTrees.jl interface with all nodes being of type Node.\n\nUse trees(booster) to return all trees in the model as Node objects, see trees.\n\nAll properties of this struct should be considered public, see propertynames(node) for a list.  Leaf nodes will have their value given by leaf.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.dump","page":"API","title":"XGBoost.dump","text":"dump(b::Booster; with_stats=false)\n\nReturn the model stored by Booster as a set of hierararchical objects (i.e. from parsed JSON). This can be used to inspect the state of the model.  See trees and Node which parse the output from this into a more useful format which satisfies the AbstractTrees interface.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.dumpraw","page":"API","title":"XGBoost.dumpraw","text":"dumpraw(b::Booster; format=\"json\", with_stats=false)\n\nDump the models stored by b to a string format.  Valid options for format are \"json\" or \"text\".  See also dump which returns the same thing as parsed JSON.\n\n\n\n\n\n","category":"function"},{"location":"api/#Default-Parameters","page":"API","title":"Default Parameters","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"regression\ncountregression\nclassification\nrandomforest","category":"page"},{"location":"api/#XGBoost.regression","page":"API","title":"XGBoost.regression","text":"regression(;kw...)\n\nDefault parameters for performing a regression.  Returns a named tuple that can be used to supply arguments in the usual way\n\nExample\n\nusing XGBoost: regression\n\nregression()  # will merely return default parameters as named tuple\n\nxgboost(X, y, 10; regression(max_depth=8)...)\nxgboost(X, y, 10; regression()..., max_depth=8)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.countregression","page":"API","title":"XGBoost.countregression","text":"countregression(;kw...)\n\nDefault parameters for performing a regression on a Poisson-distributed variable.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.classification","page":"API","title":"XGBoost.classification","text":"classification(;kw...)\n\nDefault parameters for performing a classification.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.randomforest","page":"API","title":"XGBoost.randomforest","text":"randomforest(;kw...)\n\nDefault parameters for training as a random forest.  Note that a conventional random forest would involve using these parameters with exactly 1 round of boosting, however there is nothing stopping you from boosting n random forests.\n\nParameters that are particularly relevant to random forests are:\n\nnum_parallel_tree: number of trees in the forest.\nsubsample: Sample fraction of data (occurs once per boosting iteration).\ncolsample_bynode: Sampling fraction of data on node splits.\nŒ∑: Learning rate, when set to 1 there is no shrinking of updates.\n\nSee here for more details.\n\nExamples\n\nusing XGBoost: regression, randomforest\n\nxgboost(X, y, 1; regression()..., randomforest()...)\n\n\n\n\n\n","category":"function"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"CurrentModule = XGBoost","category":"page"},{"location":"features/#Additional-Features","page":"Additional Features","title":"Additional Features","text":"","category":"section"},{"location":"features/#Introspection","page":"Additional Features","title":"Introspection","text":"","category":"section"},{"location":"features/#Feature-Importance","page":"Additional Features","title":"Feature Importance","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"This package contains a number of methods for inspecting the results of training and displaying the results in a legible way with Term.jl.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Feature importances can be computed explicitly using importance","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"For a quick and convenient summary one can use importancetable.  The output of this function is primarily intended for visual inspection but it is a Tables.jl compatible table so it can easily be converted to any tabular format.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"bst = xgboost(X, y)\n\nimp = DataFrame(importancetable(bst))","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"A convenient visualization of this table can also be seen with importancereport.  These will use assigned feature names, for example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"julia> df = DataFrame(randn(10,3), [\"kirk\", \"spock\", \"bones\"])\n10√ó3 DataFrame\n Row ‚îÇ kirk        spock      bones\n     ‚îÇ Float64     Float64    Float64\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1 ‚îÇ  0.731406   -0.53631    0.465881\n   2 ‚îÇ  0.553427   -0.787531  -0.838059\n   3 ‚îÇ  1.30724    -2.38111   -1.1979\n   4 ‚îÇ  0.0759902   0.418856   1.49618\n   5 ‚îÇ -0.426773   -0.32008   -0.773329\n   6 ‚îÇ -1.36495    -0.105646   1.08546\n   7 ‚îÇ  0.476315   -0.080163  -1.4846\n   8 ‚îÇ  0.144403    0.344307  -0.0301839\n   9 ‚îÇ  0.593969    0.165502   1.31196\n  10 ‚îÇ  2.15151     0.584925  -0.709128\n\njulia> bst = xgboost((df, randn(10)), 10)\n[ Info: XGBoost: starting training.\n[ Info: [1]     train-rmse:0.71749003518059951\n[ Info: [2]     train-rmse:0.57348349389049413\n[ Info: [3]     train-rmse:0.46118182517533174\n[ Info: [4]     train-rmse:0.37161911786076596\n[ Info: [5]     train-rmse:0.29986573085749962\n[ Info: [6]     train-rmse:0.24238347776088820\n[ Info: [7]     train-rmse:0.19544715478958452\n[ Info: [8]     train-rmse:0.15795933989281422\n[ Info: [9]     train-rmse:0.12805284613811851\n[ Info: [10]    train-rmse:0.10467078844629517\n[ Info: Training rounds complete.\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ XGBoost.Booster ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ  Features: [\"kirk\", \"spock\", \"bones\"]                                                ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ boosted rounds: 10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\njulia> importancereport(bst)\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ  feature  ‚îÇ    gain    ‚îÇ  weight  ‚îÇ   cover   ‚îÇ  total_gain  ‚îÇ  total_cover  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  \"bones\"  ‚îÇ  0.229349  ‚îÇ   17.0   ‚îÇ  7.64706  ‚îÇ   3.89893    ‚îÇ     130.0     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  \"spock\"  ‚îÇ  0.176391  ‚îÇ   18.0   ‚îÇ  4.77778  ‚îÇ   3.17503    ‚îÇ     86.0      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  \"kirk\"   ‚îÇ  0.115055  ‚îÇ   13.0   ‚îÇ  3.38462  ‚îÇ   1.49572    ‚îÇ     44.0      ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ","category":"page"},{"location":"features/#Tree-Inspection","page":"Additional Features","title":"Tree Inspection","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"The trees of a model belonging to a Booster can retrieved and directly inspected with trees which returns an array of Node objects each representing the model from a single round of boosting.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Tree objects satisfy the AbstractTrees.jl interface.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"julia> ts = trees(bst)\n10-element Vector{XGBoost.Node}:\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n XGBoost.Node(split_feature=\"f1\")\n\njulia> ts[1]\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ XGBoost.Node (id=0, depth=0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                                                                                      ‚îÇ\n‚îÇ     split_condition     yes     no     nmissing        gain         cover            ‚îÇ\n‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÇ\n‚îÇ      -0.267610937        1      2         1         0.284702361     10.0             ‚îÇ\n‚îÇ                                                                                      ‚îÇ\n‚îÇ   XGBoost Tree (from this node)                                                      ‚îÇ\n‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                                                     ‚îÇ\n‚îÇ                ‚îÇ                                                                     ‚îÇ\n‚îÇ                ‚îú‚îÄ‚îÄ f0 (1)                                                            ‚îÇ\n‚îÇ                ‚îÇ   ‚îú‚îÄ‚îÄ f0 (1)                                                        ‚îÇ\n‚îÇ                ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ (1): XGBoost.Node(leaf=0.042126134)                       ‚îÇ\n‚îÇ                ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ (2): XGBoost.Node(leaf=-0.0647352263)                     ‚îÇ\n‚îÇ                ‚îÇ   ‚îî‚îÄ‚îÄ (2): XGBoost.Node(leaf=0.0405130237)                          ‚îÇ\n‚îÇ                ‚îî‚îÄ‚îÄ (2): XGBoost.Node(leaf=-0.0718128532)                             ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ 2 children ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\njulia> using AbstractTrees; children(ts[1])\n2-element Vector{XGBoost.Node}:\n XGBoost.Node(split_feature=\"f0\")\n XGBoost.Node(leaf=-0.0718128532)","category":"page"},{"location":"features/#Setting-a-Custom-Objective-Function","page":"Additional Features","title":"Setting a Custom Objective Function","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xgboost uses a second order approximation, so to provide a custom objective functoin first and second order derivatives must be provided, see the docstring of updateone! for more details.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"While the derivatives can be provided manually, it is also easy to use a calculus package to compute them and supply them to xgboost.  Julia is notorious for having a large number of auto-differentiation packages.  To provide an example we will use one of the most popular such packages Zygote.jl","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"using Zygote, XGBoost\n\n# we use squared error loss to demonstrate\n‚Ñì(yÃÇ, y) = (yÃÇ - y)^2\n\n# we will try to fit this function\nùíª(x) = 2norm(x)^2 - norm(x)\nX = randn(100, 2)\ny = ùíª.(eachrow(X))\n\n# this is the (scalar) first derivative of the loss\n‚Ñì‚Ä≤ = (yÃÇ, y) -> gradient(Œ∂ -> ‚Ñì(Œ∂, y), yÃÇ)[1]\n\n# this is the (scalar) second derivative of the losss\n‚Ñì‚Ä≥ = (yÃÇ, y) -> gradient(Œ∂ -> ‚Ñì‚Ä≤(Œ∂, y), yÃÇ)[1]\n\n# the derivatives are the non-keyword arguments after the data,\n# keyword arguments can be provided as usual\nbst = xgboost((X, y), ‚Ñì‚Ä≤, ‚Ñì‚Ä≥, max_depth=8)","category":"page"},{"location":"features/#Caching-Data-From-External-Memory","page":"Additional Features","title":"Caching Data From External Memory","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xgboost can be used to cache memory from external memory on disk, see here.  In the Julia wrapper this is facilitated by allowing a DMatrix to be constructed from any Julia iterator with fromiterator.  The resulting DMatrix holds references to cache files which will have been created on disk.  For example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xy = [(X=randn(10,4), y=randn(10)) for i ‚àà 1:5]\ndm = XGBoost.fromiterator(DMatrix, Xy, cache_prefix=pwd())","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"will create a DMatrix that will use the present working directory to store cache files (if cache_prefix is not set this will be in /tmp).  Objects returned by the supplied iterator must have Symbol keys which can be used to supply arguments to DMatrix with :X being the key for the main matrix and :y being the key for labels (typically a NamedTuple or a Dict{Symbol,Any}).","category":"page"},{"location":"features/#Default-Parameters","page":"Additional Features","title":"Default Parameters","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"This wrapper can provide reasonable defaults for the following","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"regression\ncountregression\nclassification\nrandomforest","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Each of these merely returns a NamedTuple which can be used to supply keyword arguments to Booster or xgboost.  For example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"xgboost(X, y, 1; countregression()..., randomforest()..., num_parallel_tree=12)","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"will fit a random forest according to a Poisson likelihood fit with 12 trees.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = XGBoost","category":"page"},{"location":"#XGBoost","page":"Home","title":"XGBoost","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the Julia wrapper of the xgboost gradient boosting library.","category":"page"},{"location":"#TL;DR","page":"Home","title":"TL;DR","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using XGBoost\n\n# training set of 100 datapoints of 4 features\n(X, y) = (randn(100,4), randn(100))\n\n# create and train a gradient boosted tree model of 5 trees\nbst = xgboost((X, y), num_round=5, max_depth=6, objective=\"reg:squarederror\")\n\n# obtain model predictions\nyÃÇ = predict(bst, X)\n\n\nusing DataFrames\ndf = DataFrame(randn(100,3), [:a, :b, :y])\n\n# can accept tabular data, will keep feature names\nbst = xgboost((df[!, [:a, :b]], df.y))\n\n# display importance statistics retaining feature names\nimportancereport(bst)\n\n# return AbstractTrees.jl compatible tree objects describing the model\ntrees(bst)","category":"page"},{"location":"#Data-Input","page":"Home","title":"Data Input","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Data is passed to xgboost via the DMatrix object.  This is an AbstractMatrix{Union{Missing,Float32}} object which is primarily intended for internal use by libxgboost.  Julia AbstractArray data will automatically be wrapped in a DMatrix where appropriate, so users should mostly not have to call its constructors directly, but it may be helpful to understand the semantics for creating it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, the following are equivalent","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = randn(4,3)\npredict(bst, X) == predict(bst, DMatrix(X))","category":"page"},{"location":"","page":"Home","title":"Home","text":"The xgboost library interprets floating point NaN values as \"missing\" or \"null\" data.  missing values will automatically be converted so that the semantics of the resulting DMatrix will match that of a provided Julia matrix with Union{Missing,Real} values.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = [0 missing 1\n     1 0 missing\n     missing 1 0]\nisequal(DMatrix(X), x)  # nullity is preserved","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nDMatrix must allocate new arrays when fetching values from it.  One therefore should avoid using DMatrix directly except with XGBoost; retrieving values from this object should be considered useful mostly only for verification.","category":"page"},{"location":"#Feature-Naming-and-Tabular-Data","page":"Home","title":"Feature Naming and Tabular Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Xgboost supports the naming of features (i.e. columns of the feature matrix).  This can be useful for inspecting trained models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = randn(10,3)\n\ndm = DMatrix(X, feature_names=[\"a\", \"b\", \"c\"])\n\nXGBoost.setfeaturenames!(dm, [\"a\", \"b\", \"c\"])  # can also set after construction","category":"page"},{"location":"","page":"Home","title":"Home","text":"DMatrix can also accept tabular arguments.  These can be any table that satisfies the Tables.jl interface (e.g. a NamedTuple of same-length AbstractVectors or a DataFrame).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DataFrames\ndf = DataFrame(randn(10,3), [:a, :b, :c])\n\ny = randn(10)\n\nDMatrix(df, y)\n\ndf[!, :y] = y\nDMatrix(df, :y)  # equivalent to DMatrix(df, y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"When constructing a DMatrix from a table the feature names will automatically be set to the names of the columns (this can be overridden with the feature_names keyword argument).","category":"page"},{"location":"","page":"Home","title":"Home","text":"df = DataFrame(randn(10,3), [:a, :b, :c])\ndm = DMatrix(df)\nXGBoost.getfeaturenames(dm) == [\"a\", \"b\", \"c\"]","category":"page"},{"location":"#Label-Data","page":"Home","title":"Label Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Since xgboost is a supervised machine learning library, it will involve \"label\" or target training data.  This data is also provided to DMatrix objects and it is kept with the corresponding feature data.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LinearAlgebra\n\nùíª(x) = 2norm(x)^2 - norm(x)\n\nX = randn(100,2)\ny = ùíª.(eachrow(X))\n\nDMatrix(X, y)  # input data with features X and target y\nDMatrix((X, y))  # equivalent (to simplify function arguments)\nDMatrix(X, label=y)  # equivalent\n(dm = DMatrix(X); XGBoost.setlabel!(dm, y); dm)  # equivalent","category":"page"},{"location":"","page":"Home","title":"Home","text":"Training and initialization methods such as xgboost and Booster can accept feature and label data together as a tuple","category":"page"},{"location":"","page":"Home","title":"Home","text":"Booster((X, y))\nBooster(DMatrix(X, y)) # equivalent to above","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unlike feature data, label data can be extracted after construction of the DMatrix with XGBoost.getlabel.","category":"page"},{"location":"#Booster","page":"Home","title":"Booster","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Booster object holds model data.  They are created with training data.  Internally this is always a DMatrix but arguments will be automatically converted.","category":"page"},{"location":"#[Parameters](https://xgboost.readthedocs.io/en/stable/parameter.html)","page":"Home","title":"Parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Keyword arguments to Booster are xgboost model parameters.  These are described in detail here and should all be passed exactly as they are described in the main xgbosot documentation (in a few cases such as Greek letters we also allow unicode equivalents).","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Booster objects can be trained with update!.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ùíª(x) = 2norm(x)^2 - norm(x)\n\nX = randn(100,3)\ny = ùíª.(eachrow(X))\n\nbst = Booster((X, y), max_depth=8, Œ∑=0.5)\n\n# 20 rounds of training\nupdate!(bst, (X, y), num_round=20)\n\nyÃÇ = predict(bst, X)\n\nusing Statistics\nmean(yÃÇ - y)/std(y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Xgboost expects Boosters to be initialized with training data, therefore there is usually no need to define Booster separate from training.  A shorthand for the above, provided by xgboost is","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), num_round=20, max_depth=8, Œ∑=0.5)\n\n# feature names can also be set here\nbst = xgboost((X, y), num_round=20, feature_names=[\"a\", \"b\"], max_depth=8, Œ∑=0.5)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that Boosters can still be boosted with update! after they are create with xgboost or otherwise.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), num_round=20)","category":"page"},{"location":"","page":"Home","title":"Home","text":"is equivalent to","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), nun_round=10)\nupdate!(bst, (X, y), num_round=10)","category":"page"}]
}
