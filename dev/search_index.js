var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"CurrentModule = XGBoost","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#Data-Input","page":"API","title":"Data Input","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DMatrix\nload(::Type{DMatrix}, ::AbstractString)\nsave(::DMatrix, ::AbstractString)\nsetfeaturenames!\ngetfeaturenames\nsetinfo!\nsetinfos!\nsetlabel!\ngetinfo\nslice\nnrows\nncols\nsize(::DMatrix)\nisgpu\ngetlabel\ngetweights\nsetfeatureinfo!\ngetfeatureinfo\nsetproxy!\nDataIterator\nfromiterator","category":"page"},{"location":"api/#XGBoost.DMatrix","page":"API","title":"XGBoost.DMatrix","text":"DMatrix <: AbstractMatrix{Union{Missing,Float32}}\n\nData structure for storing data which can be understood by an xgboost Booster. These can store both features and targets.  Values of the DMatrix can be accessed as with any other AbstractMatrix, however doing so causes additional allocations.  Performant indexing and matrix operation code should not use DMatrix directly.\n\nAside from a primary array, the DMatrix object can have various \"info\" fields associated with it. Training target variables are stored as a special info field with the name label, see setinfo! and setinfos!.  These can be retrieved with getinfo and getlabel.\n\nNote that the xgboost library internally uses Float32 to represent all data, so input data is automatically copied unless provided in this format.  Unfortunately because of the different representations used by C and Julia, any non Transpose matrix will also be copied.\n\nOn missing Values\n\nXgboost supports training on missing data.  Such data is simply omitted from tree splits.  Because the DMatrix is internally a Float32 matrix, libxgboost uses a settable default value to represent missing values, see the missing_value keyword argument below (default NaN32).  This value is used only on matrix construction.  This will cause input matrix elements to ultimately be converted to missing.  The most obvious consequence of this is that NaN32 values will automatically be converted to missing with default arguments.  The provided constructors ensure that missing values will be preserved.\n\nTL;DR: DMatrix supports missing and NaN's will be converted to missing.\n\nConstructors\n\nDMatrix(X::AbstractMatrix; kw...)\nDMatrix(X::AbstractMatrix, y::AbstractVector; kw...)\nDMatrix((X, y); kw...)\nDMatrix(tbl; kw...)\nDMatrix(tbl, y; kw...)\nDMatrix(tbl, yname::Symbol; kw...)\n\nArguments\n\nX: A matrix that is the primary data wrapped by the DMatrix.  Elements can be missing.   Matrices with Float32 eleemnts do not need to be copied.\ny: Data to assign to the label info field.  This is the target variable used in training.   Can also be set with the label keyword.\ntbl: The input matrix in tabular form.  tbl must satisfy the Tables.jl interface.   If data is passed in tabular form feature names will be set automatically but can   be overriden with the keyword argument.\nyname: If passed a tabular argument tbl, yname is the name of the column which holds the   label data.  It will automatically be omitted from the features.\n\nKeyword Arguments\n\nmissing_value: The Float32 value of elements of input data to be interpreted as missing,   defaults to NaN32.\nlabel: Training target data, this is the same as the y argument above, i.e.   DMatrix(X,y) and DMatrix(X, label=y) are equivalent.\nweight: An AbstractVector of weights for each data point.  This array must have lenght   equal to the number of rows of the main data matrix.\nbase_margin: Sets the global bias for a boosted model trained on this dataset, see   https://xgboost.readthedocs.io/en/stable/prediction.html#base-margin\n\nExamples\n\n(X, y) = (randn(10,3), randn(10))\n\n# the following are all equivalent\nDMatrix(X, y)\nDMatrix((X, y))\nDMatrix(X, label=y)\n\nDMatrix(X, y, feature_names=[\"a\", \"b\", \"c\"])  # explicitly set feature names\n\ndf = DataFrame(A=randn(10), B=randn(10))\nDMatrix(df)  # has feature names [\"A\", \"B\"] but no label\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.load-Tuple{Type{DMatrix}, AbstractString}","page":"API","title":"XGBoost.load","text":"load(DMatrix, fname; silent=true, format=:libsvm, kw...)\n\nLoad a DMatrix from file with name fname.  The matrix must have been serialized with a call to save(::DMatrix, fname).  If silent the xgboost library will print logs to stdout. Additional keyword arguments are passed to the DMatrix on construction. Format describes the file format, valid options are :binary, :csv and :libsvm.\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.save-Tuple{DMatrix, AbstractString}","page":"API","title":"XGBoost.save","text":"save(dm::DMatrix, fname; silent=true)\n\nSave the DMatrix to file fname in an opaque (xgboost-specific) serialization format. Will print logs to stdout unless silent.  Files created with this function can be loaded using XGBoost.load(DMatrix, fname, format=:binary).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.setfeaturenames!","page":"API","title":"XGBoost.setfeaturenames!","text":"setfeaturenames!(dm::DMatrix, names)\n\nSets the names of the features in dm.  This can be used by Booster for reporting. names must be a rank-1 array of strings with length equal to the number of features. Note that this will be set automatically by DMatrix constructors from table objects.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getfeaturenames","page":"API","title":"XGBoost.getfeaturenames","text":"getfeaturenames(dm::DMatrix)\n\nGet the names of features in dm.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setinfo!","page":"API","title":"XGBoost.setinfo!","text":"setinfo!(dm::DMatrix, name, info)\n\nSet DMatrix ancillary info, for example :label or :weight.  name can be a string or a Symbol.  See DMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setinfos!","page":"API","title":"XGBoost.setinfos!","text":"setinfos!(dm::DMatrix; kw...)\n\nMake arbitrarily many calls to setinfo! via keyword arguments.  This function is called by all DMatrix constructors, i.e. DMatrix(X; kw...) is equivalent to setinfos!(DMatrix(X); kw...).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setlabel!","page":"API","title":"XGBoost.setlabel!","text":"setlabel!(dm::DMatrix, y)\n\nSet the label data of dm to y.  Equivalent to setinfo!(dm, \"label\", y).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getinfo","page":"API","title":"XGBoost.getinfo","text":"getinfo(dm::DMatrix, T, name)\n\nGet DMatrix info with name name.  Users must specify the underlying data type due to limitations of the xgboost library.  One must have T<:AbstractFloat to get floating point data (e.g. label, weight), or T<:Integer to get integer data.  The output will be converted to Vector{T} in all cases. name can be either a string or Symbol.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.slice","page":"API","title":"XGBoost.slice","text":"slice(dm::DMatrix, idx; kw...)\n\nCreate a new DMatrix out of the subset of rows of dm given by indices idx. For performance reasons it is recommended to take slices before converting to DMatrix. Additional keyword arguments are passed to the newly constructed slice.\n\nThis can also be called via Base.getindex, for example, the following are equivalent\n\nslice(dm, 1:4)\ndm[1:4, :]  # second argument *must* be `:` as column slices are not supported.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.nrows","page":"API","title":"XGBoost.nrows","text":"nrows(dm::DMatrix)\n\nReturns the number of rows of the DMatrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.ncols","page":"API","title":"XGBoost.ncols","text":"ncols(dm::DMatrix)\n\nReturns the number of columns of the DMatrix.  Note that this will only count columns of the main data (the X argument to the constructor).  The value returned is independent of the presence of labels. In particular size(X,2) == ncols(DMatrix(X)).\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.size-Tuple{DMatrix}","page":"API","title":"Base.size","text":"size(dm::DMatrix, [dim])\n\nReturns the size of the primary data of the DMatrix.  Note that this only accounts for the primary data and is independent of whether labels or any other ancillary data are present.  In particular size(X) == size(DMatrix(X)).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.isgpu","page":"API","title":"XGBoost.isgpu","text":"isgpu(dm::DMatrix)\n\nWhether or not the DMatrix data was initialized for a GPU.  Boosters trained on such data utilize the GPU for training.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getlabel","page":"API","title":"XGBoost.getlabel","text":"getlabel(dm::DMatrix)\n\nRetrieve the label (training target) data from the DMatrix.  Returns Float32[] if not set.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getweights","page":"API","title":"XGBoost.getweights","text":"getweights(dm::DMatrix)\n\nGet data training weights.  Returns Float32[] if not set.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setfeatureinfo!","page":"API","title":"XGBoost.setfeatureinfo!","text":"setfeatureinfo!(dm::DMatrix, info_name, strs)\n\nSets feature metadata in dm.  Valid options for info_name are \"feature_name\" and \"feature_type\". strs must be a rank-1 array of strings.  See setfeaturenames!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getfeatureinfo","page":"API","title":"XGBoost.getfeatureinfo","text":"getfeatureinfo(dm::DMatrix, info_name)\n\nGet feature info that was set via setfeatureinfo!.  Valid options for info_name are \"feature_name\" and \"feature_type\".  See getfeaturenames.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setproxy!","page":"API","title":"XGBoost.setproxy!","text":"setproxy!(dm::DMatrix, X::AbstractMatrix; kw...)\n\nSet data in a \"proxy\" DMatrix like one created with proxy(DMatrix).  Keyword arguments are set to the passed matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.DataIterator","page":"API","title":"XGBoost.DataIterator","text":"DataIterator\n\nA data structure which wraps an iterator which iteratively provides data for a DMatrix.  This can be used e.g. to aid with loading data into external memory into a DMatrix object that can be used by Booster.\n\nUsers should not typically have to deal with DataIterator directly as it is essentially a wrapper around a normal Julia iterator for the purpose of achieving compatiblity with the underlying xgboost library calls.  See fromiterator for how to construct a DMatrix from an iterator.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.fromiterator","page":"API","title":"XGBoost.fromiterator","text":"fromiterator(DMatrix, itr; cache_prefix=joinpath(tempdir(),\"xgb-cache\"), nthreads=nothing, kw...)\n\nCreate a DMatrix from an iterable object.  itr can be any object that implements Julia's Base iteration protocol.  Objects returned by the iterator must be key-value collections with Symbol keys with X as the main matrix and y as labels.  For example\n\n(X=randn(10,2), y=randn(10))\n\nOther keys will be interpreted as keyword arguments to DMatrix.\n\nWhen this is called XGBoost will start caching data provided by the iterator on disk in a format that it likes.  All cache files generated this way will have a the prefix cache_prefix which is in /tmp by default.\n\nWhat exactly xgboost does with nthreads is a bit mysterious, nothing gives the library's default.\n\nAdditional keyword arguments are passed to a DMatrix constructor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training-and-Prediction","page":"API","title":"Training and Prediction","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"xgboost\nBooster\nupdateone!\nupdate!\npredict\npredict_nocopy\nsetparam!\nsetparams!\ngetnrounds\nload!\nload(::Type{Booster}, ::AbstractString)\nsave(::Booster, ::AbstractString)\nserialize\nnfeatures\ndeserialize!\ndeserialize","category":"page"},{"location":"api/#XGBoost.xgboost","page":"API","title":"XGBoost.xgboost","text":"xgboost(data; num_round=10, watchlist=Dict(), kw...)\nxgboost(data, ℓ′, ℓ″; kw...)\n\nCreates an xgboost gradient booster object on training data data and runs nrounds of training. This is essentially an alias for constructing a Booster with data and keyword arguments followed by update! for nrounds.\n\nwatchlist is a dict the keys of which are strings giving the name of the data to watch and the values of which are DMatrix objects containing the data. It is mandatory to use an OrderedDict when utilising earlystoppingrounds and there is more than 1 element in watchlist to ensure XGBoost uses the  correct and intended dataset to perform early stop.\n\nearly_stopping_rounds activates early stopping if set to > 0. Validation metric needs to improve at  least once in every k rounds. If watchlist is not explicitly provided, it will use the training dataset  to evaluate the stopping criterion. Otherwise, it will use the last data element in watchlist and the last metric in eval_metric (if more than one). Note that watchlist cannot be empty if  early_stopping_rounds is enabled.\n\nmaximize If earlystoppingrounds is set, then this parameter must be set as well. When it is false, it means the smaller the evaluation score the better. When set to true, the larger the evaluation score the better.\n\nAll other keyword arguments are passed to Booster.  With few exceptions these are model training hyper-parameters, see here for a comprehensive list.\n\nA custom loss function can be provided via its first and second derivatives (ℓ′ and ℓ″ respectively). See updateone! for more details.\n\nExamples\n\n# Example 1: Basic usage of XGBoost\n(X, y) = (randn(100,3), randn(100))\n\nb = xgboost((X, y), num_round=10, max_depth=10, η=0.1)\n\nŷ = predict(b, X)\n\n# Example 2: Using early stopping (using a validation set) with a watchlist\ndtrain = DMatrix((randn(100,3), randn(100)))\ndvalid = DMatrix((randn(100,3), randn(100)))\n\nwatchlist = OrderedDict([\"train\" => dtrain, \"valid\" => dvalid])\n\nb = xgboost(dtrain, num_round=10, early_stopping_rounds = 2, watchlist = watchlist, max_depth=10, η=0.1)\n\n# note that ntree_limit in the predict function helps assign the upper bound for iteration_range in the XGBoost API 1.4+\nŷ = predict(b, dvalid, ntree_limit = b.best_iteration)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.Booster","page":"API","title":"XGBoost.Booster","text":"Booster\n\nData structure containing xgboost decision trees or other model objects.  Booster is used in all methods for training and predition.\n\nBooster can only consume data from DMatrix objects but most methods can convert provided data implicitly.  Note that Booster does not store any of its input or output data.\n\nSee xgboost which is shorthand for a Booster constructor followed by training.\n\nThe Booster object records all non-default model hyper-parameters set either at construction or with setparam!.  The xgboost library does not support retrieval of such parameters so these should be considered for UI purposes only, they are reported in the deafult show methods of the Booster.\n\nConstructors\n\nBooster(train; kw...)\nBooster(trains::AbstractVector; kw...)\n\nArguments\n\ntrain: Training data.  If not a DMatrix this will be passed to the DMatrix constructor.   For example it can be a training matrix or a training matrix, target pair.\ntrains: An array of objects used as training data, each of which will be passed to a DMatrix   constructor.\n\nKeyword Arguments\n\nAll keyword arguments excepting only those listed below will be interpreted as model parameters, see here for a comprehensive list. Both parameter names and their values must be provided exactly as they appear in the linked documentation.  Model parameters can also be set after construction, see setparam! and setparams!.\n\ntree_method: This parameter gets special handling.  By default it is nothing which uses the default   from libxgboost as per the documentation unless GPU arrays are used in which case it defaults to   \"gpu_hist\".  If an explicit option is set, it will always be used.\nfeature_names: Sets the feature names of training data.  This will use the feature names set in the   input data if available (e.g. if tabular data was passed this will use column names).\nmodel_buffer: A buffer (AbstractVector{UInt8} or IO) from which to load an existing booster   model object.\nmodel_file: Name of a file from which to load an existing booster model object, see save.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.updateone!","page":"API","title":"XGBoost.updateone!","text":"updateone!(b::Booster, data; round_number=getnrounds(b)+1,\n           watchlist=Dict(\"train\"=>data), update_feature_names=false\n          )\n\nRun one round of gradient boosting with booster b on data data.  data can be any object that is accepted by a DMatrix constructor.  round_number is the number of the current round and is used for logs only.  Info logs will be printed for training sets in watchlist; keys give the name of that dataset for logging purposes only.\n\n\n\n\n\nupdateone!(b::Booster, data, ℓ′, ℓ″; kw...)\n\nRun one of gradient boosting with a loss function ℓ.  ℓ′ and ℓ″ are the first and second scalar derivatives of the loss function.  For example\n\nℓ(ŷ, y) = (ŷ - y)^2\nℓ′(ŷ, y) = 2(ŷ - y)\nℓ″(ŷ, y) = 2\n\nwhere the derivatives are with respect to the first argument (the prediction).\n\nOther arguments are the same as they would be provided to other methods of updateone!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.update!","page":"API","title":"XGBoost.update!","text":"update!(b::Booster, data; num_round=1, kw...)\nupdate!(b::Booster, data, ℓ′, ℓ″; kw...)\n\nRun num_round rounds of gradient boosting on Booster b.\n\nThe first and second derivatives of the loss function (ℓ′ and ℓ″ respectively) can be provided for custom loss.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.predict","page":"API","title":"XGBoost.predict","text":"predict(b::Booster, data; margin=false, training=false, ntree_limit=0)\n\nUse the model b to run predictions on data.  This will return a Vector{Float32} which can be compared to training or test target data.\n\nIf ntree_limit > 0 only the first ntree_limit trees will be used in prediction.\n\nExamples\n\n(X, y) = (randn(100,3), randn(100))\nb = xgboost((X, y), 10)\n\nŷ = predict(b, X)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.predict_nocopy","page":"API","title":"XGBoost.predict_nocopy","text":"predict_nocopy(b::Booster, data; kw...)\n\nSame as predict, but the output array is not copied.  Data in the array output by this function may be overwritten by future calls to predict_nocopy or predict.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setparam!","page":"API","title":"XGBoost.setparam!","text":"setparam!(b::Booster, name, val)\n\nSet a model parameter in the booster.  The complete list of model parameters can be found here.  Any non-default parameters set via this method will be stored so they can be seen in REPL text output, however the xgboost library does not support parameter retrieval. name can be either a string or a Symbol.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.setparams!","page":"API","title":"XGBoost.setparams!","text":"setparams!(b::Booster; kw...)\n\nSet arbitrarily many model parameters via keyword arguments, see setparam!.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.getnrounds","page":"API","title":"XGBoost.getnrounds","text":"getnrounds(b::Booster)\n\nGet the number of rounds run by the Booster object.  Normally this will correspond to the total number of trees stored in the Booster.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.load!","page":"API","title":"XGBoost.load!","text":"load!(b::Booster, file_or_buffer)\n\nLoad a serialized Booster object from a file or buffer into an existing model object. file_or_buffer can be a string giving the name of the file to load from, a AbstractVector{UInt8} buffer, or an IO.\n\nThis should load models stored via save (not serialize which may give incompatible buffers).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.load-Tuple{Type{Booster}, AbstractString}","page":"API","title":"XGBoost.load","text":"load(Booster, file_or_buffer)\n\nLoad a saved Booster model object from a file or buffer. file_or_buffer can be a string giving the name of the file to load from, an AbstractVector{UInt8} buffer or an IO.\n\nThis should load models stored via save (not serialize which may give incompatible buffers).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.save-Tuple{Booster, AbstractString}","page":"API","title":"XGBoost.save","text":"save(b::Booster, fname; format=\"json\")\nsave(b::Booster, Vector{UInt8}; format=\"json\")\nsave(b::Booster, io::IO; format=\"json\")\n\nSave the Booster object.  This saves to formats which are intended to be stored on disk but the formats used are a lot zanier than those used by deserialize. A model saved with this function can be retrieved with load or load!. Valid formats are \"json\" and \"ubj\" (universal binary JSON).\n\n\n\n\n\n","category":"method"},{"location":"api/#XGBoost.serialize","page":"API","title":"XGBoost.serialize","text":"serialize(b::Booster)\n\nSerialize the model b into an opaque binary format.  Returns a Vector{UInt8}. The output of this function can be loaded with deserialize.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.nfeatures","page":"API","title":"XGBoost.nfeatures","text":"nfeatures(b::Booster)\n\nGet the number of features on which b is being trained.  Note that this can return nothing if the Booster object is uninitialized (was created with no data arguments).\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.deserialize!","page":"API","title":"XGBoost.deserialize!","text":"deserialize!(b::Booster, buf)\n\nDeserialize a buffer created with serialize to the provided Booster object.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.deserialize","page":"API","title":"XGBoost.deserialize","text":"deserialize(Booster, buf, data=[]; kw...)\n\nDeserialize the data in buffer buf to a new Booster object.  The data in buf should have been created with serialize.  data and keyword arguments are sent to a Booster constructor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Introspection","page":"API","title":"Introspection","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"trees\nimportancetable\nimportance\nimportancereport\nNode\ndump\ndumpraw","category":"page"},{"location":"api/#XGBoost.trees","page":"API","title":"XGBoost.trees","text":"trees(b::Booster; with_stats=true)\n\nReturn all trees of the model of the Booster b as Node objects.  The output of this function is a Vector of Nodes each representing the root of a separate tree.\n\nIf with_stats the output Node objects will contain the computed statistics gain and cover.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importancetable","page":"API","title":"XGBoost.importancetable","text":"importancetable(b::Booster)\n\nReturn a Table.jl compatible table (named tuple of Vectors) giving a summary of all available feature importance statistics for b.  This table is mainly intended for display purposes, see importance for a more direct way of retrieving importance statistics. See importancereport for a convenient display of this table.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importance","page":"API","title":"XGBoost.importance","text":"importance(b::Booster, type=\"gain\")\n\nCompute feature importance metric from a trained Booster. Valid options for type are\n\n\"gain\"\n\"weight\"\n\"cover\"\n\"total_gain\"\n\"total_cover\"\n\nThe output is an OrderedDict with keys corresponding to feature names and values corresponding to importances.  The importances are always returned as Vectors, typically with length 1 but possibly longer in multi-class cases.  If feature names were not set the keys of the output dict will be integers giving the feature column number.  The output will be sorted with the highest importance feature listed first and the lowest importance feature listed last.\n\nSee importancetable for a way to generate a tabular summary of all available feature importances and importancereport for a convenient text display of it.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.importancereport","page":"API","title":"XGBoost.importancereport","text":"importancereport(b::Booster)\n\nShow a convenient text display of the table output by importancetable.\n\nThis is intended entirely for display purposes, see importance for how to retrieve feature importance statistics directly.\n\nnote: Note\nIn Julia >= 1.9, you have to load Term.jl to be able to use this functionality.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.Node","page":"API","title":"XGBoost.Node","text":"Node\n\nA data structure representing a node of an XGBoost tree. These are constructed from the dicts returned by dump.\n\nNodes satisfy the AbstractTrees.jl interface with all nodes being of type Node.\n\nUse trees(booster) to return all trees in the model as Node objects, see trees.\n\nAll properties of this struct should be considered public, see propertynames(node) for a list.  Leaf nodes will have their value given by leaf.\n\n\n\n\n\n","category":"type"},{"location":"api/#XGBoost.dump","page":"API","title":"XGBoost.dump","text":"dump(b::Booster; with_stats=false)\n\nReturn the model stored by Booster as a set of hierararchical objects (i.e. from parsed JSON). This can be used to inspect the state of the model.  See trees and Node which parse the output from this into a more useful format which satisfies the AbstractTrees interface.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.dumpraw","page":"API","title":"XGBoost.dumpraw","text":"dumpraw(b::Booster; format=\"json\", with_stats=false)\n\nDump the models stored by b to a string format.  Valid options for format are \"json\" or \"text\".  See also dump which returns the same thing as parsed JSON.\n\n\n\n\n\n","category":"function"},{"location":"api/#Default-Parameters","page":"API","title":"Default Parameters","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"regression\ncountregression\nclassification\nrandomforest","category":"page"},{"location":"api/#XGBoost.regression","page":"API","title":"XGBoost.regression","text":"regression(;kw...)\n\nDefault parameters for performing a regression.  Returns a named tuple that can be used to supply arguments in the usual way\n\nExample\n\nusing XGBoost: regression\n\nregression()  # will merely return default parameters as named tuple\n\nxgboost(X, y, 10; regression(max_depth=8)...)\nxgboost(X, y, 10; regression()..., max_depth=8)\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.countregression","page":"API","title":"XGBoost.countregression","text":"countregression(;kw...)\n\nDefault parameters for performing a regression on a Poisson-distributed variable.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.classification","page":"API","title":"XGBoost.classification","text":"classification(;kw...)\n\nDefault parameters for performing a classification.\n\n\n\n\n\n","category":"function"},{"location":"api/#XGBoost.randomforest","page":"API","title":"XGBoost.randomforest","text":"randomforest(;kw...)\n\nDefault parameters for training as a random forest.  Note that a conventional random forest would involve using these parameters with exactly 1 round of boosting, however there is nothing stopping you from boosting n random forests.\n\nParameters that are particularly relevant to random forests are:\n\nnum_parallel_tree: number of trees in the forest.\nsubsample: Sample fraction of data (occurs once per boosting iteration).\ncolsample_bynode: Sampling fraction of data on node splits.\nη: Learning rate, when set to 1 there is no shrinking of updates.\n\nSee here for more details.\n\nExamples\n\nusing XGBoost: regression, randomforest\n\nxgboost(X, y, 1; regression()..., randomforest()...)\n\n\n\n\n\n","category":"function"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"CurrentModule = XGBoost","category":"page"},{"location":"features/#Additional-Features","page":"Additional Features","title":"Additional Features","text":"","category":"section"},{"location":"features/#Introspection","page":"Additional Features","title":"Introspection","text":"","category":"section"},{"location":"features/#Feature-Importance","page":"Additional Features","title":"Feature Importance","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"This package contains a number of methods for inspecting the results of training and displaying the results.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Feature importances can be computed explicitly using importance","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"For a quick and convenient summary one can use importancetable.  The output of this function is primarily intended for visual inspection but it is a Tables.jl compatible table so it can easily be converted to any tabular format.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"bst = xgboost(X, y)\n\nimp = DataFrame(importancetable(bst))","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"XGBoost also supports rich terminal output with Term.jl. A convenient visualization of this table can also be seen with importancereport.  These will use assigned feature names, for example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"julia> df = DataFrame(randn(10,3), [\"kirk\", \"spock\", \"bones\"])\n10×3 DataFrame\n Row │ kirk       spock       bones\n     │ Float64    Float64     Float64\n─────┼───────────────────────────────────\n   1 │  0.663934  -0.419345   -0.489801\n   2 │  1.19064    0.420935   -0.321852\n   3 │  0.713867   0.293724    0.0450463\n   4 │ -1.3474    -0.402996    1.50831\n   5 │ -0.458164   0.0399281  -0.83443\n   6 │ -0.277555   0.149485    0.408656\n   7 │ -1.79885   -1.1535      0.99213\n   8 │ -0.177408  -0.818639    0.280188\n   9 │ -1.26053   -1.60734     2.21421\n  10 │  0.30378   -0.299256    0.384029\n\njulia> bst = xgboost((df, randn(10)), num_round=10);\n[ Info: XGBoost: starting training.\n[ Info: [1]     train-rmse:0.57998637329114211\n[ Info: [2]     train-rmse:0.48232409595403752\n[ Info: [3]     train-rmse:0.40593080843433427\n[ Info: [4]     train-rmse:0.34595769369793850\n[ Info: [5]     train-rmse:0.29282108263987289\n[ Info: [6]     train-rmse:0.24862819795032731\n[ Info: [7]     train-rmse:0.21094418685218519\n[ Info: [8]     train-rmse:0.17903024616536045\n[ Info: [9]     train-rmse:0.15198720040980171\n[ Info: [10]    train-rmse:0.12906074380448287\n[ Info: Training rounds complete.\n\njulia> using Term; Panel(bst)\n╭──── XGBoost.Booster ─────────────────────────────────────────────────────────────────╮\n│  Features: [\"kirk\", \"spock\", \"bones\"]                                                │\n│                                                                                      │\n│          Parameter          Value                                                    │\n│   ─────────────────────────────────                                                  │\n│     validate_parameters     true                                                     │\n│                                                                                      │\n╰──── boosted rounds: 10 ──────────────────────────────────────────────────────────────╯\n\njulia> importancereport(bst)\n╭───────────┬─────────────┬──────────┬───────────┬──────────────┬───────────────╮\n│  feature  │    gain     │  weight  │   cover   │  total_gain  │  total_cover  │\n├───────────┼─────────────┼──────────┼───────────┼──────────────┼───────────────┤\n│  \"bones\"  │  0.358836   │   15.0   │  8.53333  │   5.38254    │     128.0     │\n├───────────┼─────────────┼──────────┼───────────┼──────────────┼───────────────┤\n│  \"spock\"  │  0.157437   │   16.0   │   4.75    │   2.51899    │     76.0      │\n├───────────┼─────────────┼──────────┼───────────┼──────────────┼───────────────┤\n│  \"kirk\"   │  0.0128546  │   34.0   │  2.91176  │   0.437056   │     99.0      │\n╰───────────┴─────────────┴──────────┴───────────┴──────────────┴───────────────╯","category":"page"},{"location":"features/#Tree-Inspection","page":"Additional Features","title":"Tree Inspection","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"The trees of a model belonging to a Booster can retrieved and directly inspected with trees which returns an array of Node objects each representing the model from a single round of boosting.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Tree objects satisfy the AbstractTrees.jl interface.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"julia> ts = trees(bst)\n10-element Vector{XGBoost.Node}:\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"bones\")\n\njulia> using Term; Panel(ts[1])\n╭──── XGBoost.Node (id=0, depth=0) ────────────────────────────────────────────────────╮\n│                                                                                      │\n│     split_condition     yes     no     nmissing        gain        cover             │\n│   ────────────────────────────────────────────────────────────────────────           │\n│       0.396342576        1      2         1         1.86042714     10.0              │\n│                                                                                      │\n│   XGBoost Tree (from this node)                                                      │\n│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                     │\n│                │                                                                     │\n│                ├── bones < 0.396                                                     │\n│                │   ├── bones < 0.332: XGBoost.Node(leaf=-0.159539297)                │\n│                │   └── bones ≥ 0.332: XGBoost.Node(leaf=-0.0306737479)               │\n│                └── bones ≥ 0.396                                                     │\n│                    ├── spock < -0.778                                                │\n│                    │   ├── kirk < -1.53: XGBoost.Node(leaf=-0.0544514731)            │\n│                    │   └── kirk ≥ -1.53: XGBoost.Node(leaf=0.00967349485)            │\n│                    └── spock ≥ -0.778                                                │\n│                        ├── kirk < -0.812: XGBoost.Node(leaf=0.0550933369)            │\n│                        └── kirk ≥ -0.812: XGBoost.Node(leaf=0.228843644)             │\n╰──── 2 children ──────────────────────────────────────────────────────────────────────╯\n\njulia> using AbstractTrees; children(ts[1])\n2-element Vector{XGBoost.Node}:\n XGBoost.Node(split_feature=\"bones\")\n XGBoost.Node(split_feature=\"spock\")","category":"page"},{"location":"features/#Setting-a-Custom-Objective-Function","page":"Additional Features","title":"Setting a Custom Objective Function","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xgboost uses a second order approximation, so to provide a custom objective functoin first and second order derivatives must be provided, see the docstring of updateone! for more details.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"While the derivatives can be provided manually, it is also easy to use a calculus package to compute them and supply them to xgboost.  Julia is notorious for having a large number of auto-differentiation packages.  To provide an example we will use one of the most popular such packages Zygote.jl","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"using Zygote, XGBoost\n\n# we use squared error loss to demonstrate\nℓ(ŷ, y) = (ŷ - y)^2\n\n# we will try to fit this function\n𝒻(x) = 2norm(x)^2 - norm(x)\nX = randn(100, 2)\ny = 𝒻.(eachrow(X))\n\n# this is the (scalar) first derivative of the loss\nℓ′ = (ŷ, y) -> gradient(ζ -> ℓ(ζ, y), ŷ)[1]\n\n# this is the (scalar) second derivative of the losss\nℓ″ = (ŷ, y) -> gradient(ζ -> ℓ′(ζ, y), ŷ)[1]\n\n# the derivatives are the non-keyword arguments after the data,\n# keyword arguments can be provided as usual\nbst = xgboost((X, y), ℓ′, ℓ″, max_depth=8)","category":"page"},{"location":"features/#Caching-Data-From-External-Memory","page":"Additional Features","title":"Caching Data From External Memory","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xgboost can be used to cache memory from external memory on disk, see here.  In the Julia wrapper this is facilitated by allowing a DMatrix to be constructed from any Julia iterator with fromiterator.  The resulting DMatrix holds references to cache files which will have been created on disk.  For example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Xy = [(X=randn(10,4), y=randn(10)) for i ∈ 1:5]\ndm = XGBoost.fromiterator(DMatrix, Xy, cache_prefix=pwd())","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"will create a DMatrix that will use the present working directory to store cache files (if cache_prefix is not set this will be in /tmp).  Objects returned by the supplied iterator must have Symbol keys which can be used to supply arguments to DMatrix with :X being the key for the main matrix and :y being the key for labels (typically a NamedTuple or a Dict{Symbol,Any}).","category":"page"},{"location":"features/#Default-Parameters","page":"Additional Features","title":"Default Parameters","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"This wrapper can provide reasonable defaults for the following","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"regression\ncountregression\nclassification\nrandomforest","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Each of these merely returns a NamedTuple which can be used to supply keyword arguments to Booster or xgboost.  For example","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"xgboost(X, y, 1; countregression()..., randomforest()..., num_parallel_tree=12)","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"will fit a random forest according to a Poisson likelihood fit with 12 trees.","category":"page"},{"location":"features/#GPU-Support","page":"Additional Features","title":"GPU Support","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"XGBoost supports GPU-assisted training on Nvidia GPU's with CUDA via CUDA.jl.  To utilize the GPU, one has to load CUDA and construct a DMatrix object from GPU arrays.  There are two ways of doing this:","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"Pass a CuArray as the training matrix (conventionally X, the first argument to DMatrix).\nPass a table with all columns as CuVectors.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"You can check whether a DMatrix can use the GPU with XGBoost.isgpu.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"The target or label data does not need to be a CuArray.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"It is not necessary to create an explicit DMatrix to use GPU features, one can pass the data normally directly to xgboost or Booster, as long as that data consists of CuArrays.","category":"page"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"note: Note\nThe tree_method parameter to Booster has special handling.  If nothing, it will use libxgboost defaults as per the documentation, unless a GPU array is given in which case it will default to gpu_hist.  An explicitly set value will override this.","category":"page"},{"location":"features/#Example","page":"Additional Features","title":"Example","text":"","category":"section"},{"location":"features/","page":"Additional Features","title":"Additional Features","text":"using CUDA\n\nX = cu(randn(1000, 3))\ny = randn(1000)\n\ndm = DMatrix(X, y)\nXGBoost.isgpu(dm)  # true\n\nX = (x1=cu(randn(1000)), x2=cu(randn(1000)))\ndm = DMatrix(X, y)\nXGBoost.isgpu(dm)  # true\n\nxgboost((X, y), num_round=10)  # no need to use `DMatrix`","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = XGBoost","category":"page"},{"location":"#XGBoost","page":"Home","title":"XGBoost","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the Julia wrapper of the xgboost gradient boosting library.","category":"page"},{"location":"#TL;DR","page":"Home","title":"TL;DR","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using XGBoost\n\n# training set of 100 datapoints of 4 features\n(X, y) = (randn(100,4), randn(100))\n\n# create and train a gradient boosted tree model of 5 trees\nbst = xgboost((X, y), num_round=5, max_depth=6, objective=\"reg:squarederror\")\n\n# obtain model predictions\nŷ = predict(bst, X)\n\n\nusing DataFrames\ndf = DataFrame(randn(100,3), [:a, :b, :y])\n\n# can accept tabular data, will keep feature names\nbst = xgboost((df[!, [:a, :b]], df.y))\n\n# display importance statistics retaining feature names\nimportancereport(bst)\n\n# return AbstractTrees.jl compatible tree objects describing the model\ntrees(bst)","category":"page"},{"location":"#Data-Input","page":"Home","title":"Data Input","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Data is passed to xgboost via the DMatrix object.  This is an AbstractMatrix{Union{Missing,Float32}} object which is primarily intended for internal use by libxgboost.  Julia AbstractArray data will automatically be wrapped in a DMatrix where appropriate, so users should mostly not have to call its constructors directly, but it may be helpful to understand the semantics for creating it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, the following are equivalent","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = randn(4,3)\npredict(bst, X) == predict(bst, DMatrix(X))","category":"page"},{"location":"","page":"Home","title":"Home","text":"The xgboost library interprets floating point NaN values as \"missing\" or \"null\" data.  missing values will automatically be converted so that the semantics of the resulting DMatrix will match that of a provided Julia matrix with Union{Missing,Real} values.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = [0 missing 1\n     1 0 missing\n     missing 1 0]\nisequal(DMatrix(X), x)  # nullity is preserved","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nDMatrix must allocate new arrays when fetching values from it.  One therefore should avoid using DMatrix directly except with XGBoost; retrieving values from this object should be considered useful mostly only for verification.","category":"page"},{"location":"#Feature-Naming-and-Tabular-Data","page":"Home","title":"Feature Naming and Tabular Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Xgboost supports the naming of features (i.e. columns of the feature matrix).  This can be useful for inspecting trained models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = randn(10,3)\n\ndm = DMatrix(X, feature_names=[\"a\", \"b\", \"c\"])\n\nXGBoost.setfeaturenames!(dm, [\"a\", \"b\", \"c\"])  # can also set after construction","category":"page"},{"location":"","page":"Home","title":"Home","text":"DMatrix can also accept tabular arguments.  These can be any table that satisfies the Tables.jl interface (e.g. a NamedTuple of same-length AbstractVectors or a DataFrame).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DataFrames\ndf = DataFrame(randn(10,3), [:a, :b, :c])\n\ny = randn(10)\n\nDMatrix(df, y)\n\ndf[!, :y] = y\nDMatrix(df, :y)  # equivalent to DMatrix(df, y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"When constructing a DMatrix from a table the feature names will automatically be set to the names of the columns (this can be overridden with the feature_names keyword argument).","category":"page"},{"location":"","page":"Home","title":"Home","text":"df = DataFrame(randn(10,3), [:a, :b, :c])\ndm = DMatrix(df)\nXGBoost.getfeaturenames(dm) == [\"a\", \"b\", \"c\"]","category":"page"},{"location":"#Label-Data","page":"Home","title":"Label Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Since xgboost is a supervised machine learning library, it will involve \"label\" or target training data.  This data is also provided to DMatrix objects and it is kept with the corresponding feature data.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LinearAlgebra\n\n𝒻(x) = 2norm(x)^2 - norm(x)\n\nX = randn(100,2)\ny = 𝒻.(eachrow(X))\n\nDMatrix(X, y)  # input data with features X and target y\nDMatrix((X, y))  # equivalent (to simplify function arguments)\nDMatrix(X, label=y)  # equivalent\n(dm = DMatrix(X); XGBoost.setlabel!(dm, y); dm)  # equivalent","category":"page"},{"location":"","page":"Home","title":"Home","text":"Training and initialization methods such as xgboost and Booster can accept feature and label data together as a tuple","category":"page"},{"location":"","page":"Home","title":"Home","text":"Booster((X, y))\nBooster(DMatrix(X, y)) # equivalent to above","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unlike feature data, label data can be extracted after construction of the DMatrix with XGBoost.getlabel.","category":"page"},{"location":"#Booster","page":"Home","title":"Booster","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Booster object holds model data.  They are created with training data.  Internally this is always a DMatrix but arguments will be automatically converted.","category":"page"},{"location":"#[Parameters](https://xgboost.readthedocs.io/en/stable/parameter.html)","page":"Home","title":"Parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Keyword arguments to Booster are xgboost model parameters.  These are described in detail here and should all be passed exactly as they are described in the main xgbosot documentation (in a few cases such as Greek letters we also allow unicode equivalents).","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThe tree_method parameter has special handling.  If nothing, it will use libxgboost defaults as per the documentation, unless a GPU array is input in which case it will default to gpu_hist.  An explicitly set value will override this.","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Booster objects can be trained with update!.","category":"page"},{"location":"","page":"Home","title":"Home","text":"𝒻(x) = 2norm(x)^2 - norm(x)\n\nX = randn(100,3)\ny = 𝒻.(eachrow(X))\n\nbst = Booster((X, y), max_depth=8, η=0.5)\n\n# 20 rounds of training\nupdate!(bst, (X, y), num_round=20)\n\nŷ = predict(bst, X)\n\nusing Statistics\nmean(ŷ - y)/std(y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Xgboost expects Boosters to be initialized with training data, therefore there is usually no need to define Booster separate from training.  A shorthand for the above, provided by xgboost is","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), num_round=20, max_depth=8, η=0.5)\n\n# feature names can also be set here\nbst = xgboost((X, y), num_round=20, feature_names=[\"a\", \"b\"], max_depth=8, η=0.5)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that Boosters can still be boosted with update! after they are create with xgboost or otherwise.  For example","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), num_round=20)","category":"page"},{"location":"","page":"Home","title":"Home","text":"is equivalent to","category":"page"},{"location":"","page":"Home","title":"Home","text":"bst = xgboost((X, y), num_round=10)\nupdate!(bst, (X, y), num_round=10)","category":"page"},{"location":"#Early-Stopping","page":"Home","title":"Early Stopping","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To help prevent overfitting to the training set, it is helpful to use a validation set to evaluate against to ensure that the XGBoost iterations continue to generalise outside training loss reduction. Early stopping provides a convenient way to automatically stop the boosting process if it's observed that the generalisation capability of the model does not improve for k rounds.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If there is more than one element in watchlist, by default the last element will be used. In this case, you must use an ordered data structure (OrderedDict) compared to a standard unordered dictionary otherwise an exception will be generated. There will be a warning if you want to execute early stopping mechanism (early_stopping_rounds > 0) but have provided a watchlist with type Dict with more than 1 element.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Similarly, if there is more than one element in eval_metric, by default the last element will be used.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LinearAlgebra\nusing OrderedCollections\n\n𝒻(x) = 2norm(x)^2 - norm(x)\n\nX = randn(100,3)\ny = 𝒻.(eachrow(X))\n\ndtrain = DMatrix((X, y))\n\nX_valid = randn(50,3)\ny_valid = 𝒻.(eachrow(X_valid))\n\ndvalid = DMatrix((X_valid, y_valid))\n\nbst = xgboost(dtrain, num_round = 100, eval_metric = \"rmse\", watchlist = OrderedDict([\"train\" => dtrain, \"eval\" => dvalid]), early_stopping_rounds = 5, max_depth=6, η=0.3)\n\n# get the best iteration and use it for prediction\nŷ = predict(bst, X_valid, ntree_limit = bst.best_iteration)\n\nusing Statistics\nprintln(\"RMSE from model prediction $(round((mean((ŷ - y_valid).^2).^0.5), digits = 8)).\")\n\n# we can also retain / use the best score (based on eval_metric) which is stored in the booster\nprintln(\"Best RMSE from model training $(round((bst.best_score), digits = 8)).\")","category":"page"}]
}
